{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50e321d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading SMAP_L2_SM_P_36335_D_20211120T002559_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36335_D_20211120T002559_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36336_A_20211120T011510_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36336_A_20211120T011510_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36336_D_20211120T020425_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36336_D_20211120T020425_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36337_A_20211120T025340_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36337_A_20211120T025340_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36337_D_20211120T034255_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36337_D_20211120T034255_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36338_A_20211120T043205_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36338_A_20211120T043205_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36338_D_20211120T052120_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36338_D_20211120T052120_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36339_A_20211120T061035_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36339_A_20211120T061035_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36339_D_20211120T065950_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36339_D_20211120T065950_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36340_A_20211120T074901_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36340_A_20211120T074901_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36340_D_20211120T083816_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36340_D_20211120T083816_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36341_A_20211120T092730_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36341_A_20211120T092730_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36341_D_20211120T101645_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36341_D_20211120T101645_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36342_A_20211120T110556_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36342_A_20211120T110556_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36342_D_20211120T115511_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36342_D_20211120T115511_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36343_A_20211120T124426_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36343_A_20211120T124426_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36343_D_20211120T133341_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36343_D_20211120T133341_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36344_A_20211120T142251_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36344_A_20211120T142251_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36344_D_20211120T151206_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36344_D_20211120T151206_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36345_A_20211120T160121_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36345_A_20211120T160121_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36345_D_20211120T165036_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36345_D_20211120T165036_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36346_A_20211120T173947_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36346_A_20211120T173947_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36346_D_20211120T182901_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36346_D_20211120T182901_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36347_A_20211120T191816_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36347_A_20211120T191816_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36347_D_20211120T200731_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36347_D_20211120T200731_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36348_A_20211120T205642_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36348_A_20211120T205642_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36348_D_20211120T214557_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36348_D_20211120T214557_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36349_A_20211120T223512_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36349_A_20211120T223512_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36349_D_20211120T232427_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36349_D_20211120T232427_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36350_A_20211121T001337_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36350_A_20211121T001337_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36350_D_20211121T010252_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36350_D_20211121T010252_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36351_A_20211121T015207_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36351_A_20211121T015207_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36351_D_20211121T024122_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36351_D_20211121T024122_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36352_A_20211121T033033_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36352_A_20211121T033033_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36352_D_20211121T041947_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36352_D_20211121T041947_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36353_A_20211121T050902_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36353_A_20211121T050902_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36353_D_20211121T055817_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36353_D_20211121T055817_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36354_A_20211121T064728_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36354_A_20211121T064728_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36354_D_20211121T073643_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36354_D_20211121T073643_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36355_A_20211121T082558_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36355_A_20211121T082558_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36355_D_20211121T091512_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36355_D_20211121T091512_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36356_A_20211121T100423_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36356_A_20211121T100423_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36356_D_20211121T105338_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36356_D_20211121T105338_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36357_A_20211121T114253_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36357_A_20211121T114253_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36357_D_20211121T123208_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36357_D_20211121T123208_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36358_A_20211121T132119_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36358_A_20211121T132119_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36358_D_20211121T141033_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36358_D_20211121T141033_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36359_A_20211121T145948_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36359_A_20211121T145948_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36359_D_20211121T154903_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36359_D_20211121T154903_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36360_A_20211121T163814_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36360_A_20211121T163814_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36360_D_20211121T172729_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36360_D_20211121T172729_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36361_A_20211121T181644_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36361_A_20211121T181644_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36361_D_20211121T190554_R18290_001.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping download for SMAP_L2_SM_P_36361_D_20211121T190554_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36362_A_20211121T195509_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36362_A_20211121T195509_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36362_D_20211121T204424_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36362_D_20211121T204424_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36363_A_20211121T213339_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36363_A_20211121T213339_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36363_D_20211121T222250_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36363_D_20211121T222250_R18290_001.h5 as it already exists.\n",
      "Downloading SMAP_L2_SM_P_36364_A_20211121T231204_R18290_001.h5\n",
      "Skipping download for SMAP_L2_SM_P_36364_A_20211121T231204_R18290_001.h5 as it already exists.\n"
     ]
    }
   ],
   "source": [
    "#Final code - Downloading and deleting the h5 file after use, skipping missing day data and adding date in the dataframe - 3 august\n",
    "from datetime import datetime, timedelta\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import xarray as xr\n",
    "from io import BytesIO\n",
    "import requests\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import urllib.request\n",
    "import h5py\n",
    "import os\n",
    "import pandas as pd\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "def wait_for_downloads(download_path):\n",
    "    while any(filename.endswith(\".crdownload\") for filename in os.listdir(download_path)):\n",
    "        print(\"Waiting for downloads to complete...\")\n",
    "        time.sleep(5)\n",
    "def get_chrome_options(download_path):\n",
    "    options = Options()\n",
    "    options.add_experimental_option('prefs', {\n",
    "        'download.default_directory': download_path,\n",
    "        'download.prompt_for_download': False,\n",
    "        'download.directory_upgrade': True,\n",
    "        'safebrowsing.enabled': True\n",
    "    })\n",
    "    return options\n",
    "#download_path = \"F:\\\\soil_moisture\\\\may30\"\n",
    "def download_h5_files(url, download_path,date_str1):\n",
    "    # Initialize Chrome WebDriver with download options\n",
    "    chrome_options = get_chrome_options(download_path)\n",
    "    driver = webdriver.Chrome(executable_path=\"C:\\\\Users\\\\Acer\\\\Desktop\\\\chrome driver\\\\chromedriver-win64\\\\chromedriver.exe\", options=chrome_options)\n",
    "    driver.maximize_window()\n",
    "    driver.get(url)\n",
    "\n",
    "    # Find the username and password input elements and fill them with your credentials\n",
    "    username_input = driver.find_element_by_name(\"username\")  # Replace \"username\" with the actual name of the username input field\n",
    "    password_input = driver.find_element_by_name(\"password\")  # Replace \"password\" with the actual name of the password input field\n",
    "    username = \"toliresearch\"  # Enter your username\n",
    "    password = \"Researchgarnetoli4#\"  # Enter your password\n",
    "    username_input.send_keys(username)\n",
    "    password_input.send_keys(password)\n",
    "\n",
    "    # Submit the login form\n",
    "    login_button = driver.find_element(By.NAME, \"commit\")  # Replace 'login_button_id' with the actual ID of the login button element\n",
    "    login_button.click()\n",
    "    \n",
    "    # Wait for the visibility of the links\n",
    "    wait = WebDriverWait(driver, 20)\n",
    "    #element = wait.until(EC.visibility_of_element_located((By.LINK_TEXT, \"2015.04.11/\")))\n",
    "    try:\n",
    "        element = wait.until(EC.visibility_of_element_located((By.LINK_TEXT, date_str1)))\n",
    "        element.click()\n",
    "\n",
    "    # ... Rest of the code for downloading the .h5 files ...\n",
    "    except TimeoutException:\n",
    "        print(f\"Data not available for {date_str1}. Skipping to the next date.\")\n",
    "        driver.quit()\n",
    "        return  # Skip the rest of the function for this date\n",
    "    \n",
    "    #element = wait.until(EC.visibility_of_element_located((By.LINK_TEXT, date_str1)))\n",
    "    #element.click()\n",
    "\n",
    "    # Get all the links to .h5 files\n",
    "    links = driver.find_elements(By.XPATH, \"//a[contains(@href, '.h5')]\")\n",
    "\n",
    "    # Create an empty set to keep track of downloaded file names\n",
    "    downloaded_files = set()\n",
    "\n",
    "# Loop through the links and download the files\n",
    "    for link in links:\n",
    "        file_url = link.get_attribute(\"href\")\n",
    "        file_name = os.path.basename(file_url)\n",
    "             \n",
    "        \n",
    "        # Check if the file has the .h5 extension, if not, skip to the next link\n",
    "        if not file_name.endswith(\".h5\"):\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(download_path, file_name)\n",
    "        \n",
    "        # Check if the file has already been downloaded, if yes, skip to the next date\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"Skipping download for {file_name} as it already exists.\")\n",
    "            continue\n",
    "        elif file_name in downloaded_files:\n",
    "            continue\n",
    "        else:\n",
    "            try:\n",
    "                print(f\"Downloading {file_name}\")\n",
    "                link.click()\n",
    "                #urllib.request.urlretrieve(file_url, file_path)\n",
    "                # Add a delay to avoid overloading the server\n",
    "                #wait.until(EC.presence_of_element_located((By.XPATH, \"//span[contains(text(), 'Download complete')]\")))\n",
    "                #print(f\"{file_name} download complete\")\n",
    "                time.sleep(10)\n",
    "                downloaded_files.add(file_name)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download {file_name}: {e}\")\n",
    "    wait_for_downloads(download_path)\n",
    "    driver.quit()\n",
    "    #time.sleep(120)\n",
    "   # Replace 'folder_path' with the path to the folder containing the .h5 files\n",
    "    folder_path = download_path #\"F:\\\\soil_moisture\\\\june11\"\n",
    "\n",
    "    # Create an empty list to store DataFrames from each file\n",
    "    dfs = []\n",
    "\n",
    "    # Loop through all files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.h5'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "            with h5py.File(file_path, 'r') as h5_file:\n",
    "                dataset_path = \"Soil_Moisture_Retrieval_Data/longitude_centroid\"\n",
    "                dataset_path1 = \"Soil_Moisture_Retrieval_Data/latitude_centroid\"\n",
    "                dataset_path2 = \"Soil_Moisture_Retrieval_Data/soil_moisture\"\n",
    "                dataset_path3 = \"Soil_Moisture_Retrieval_Data/soil_moisture_error\"\n",
    "                dataset_path4 = \"Soil_Moisture_Retrieval_Data/soil_moisture_option1\"\n",
    "                dataset_path5 = \"Soil_Moisture_Retrieval_Data/soil_moisture_option2\"\n",
    "                dataset_path6 = \"Soil_Moisture_Retrieval_Data/latitude\"\n",
    "                dataset_path7 = \"Soil_Moisture_Retrieval_Data/longitude\"\n",
    "\n",
    "                # Check if the datasets exist in the file\n",
    "                if dataset_path in h5_file and dataset_path1 in h5_file and dataset_path2 in h5_file:\n",
    "                    # Access the datasets using their paths\n",
    "                    dataset = h5_file[dataset_path]\n",
    "                    dataset1 = h5_file[dataset_path1]\n",
    "                    dataset2 = h5_file[dataset_path2]\n",
    "                    dataset3 = h5_file[dataset_path3]\n",
    "                    dataset4 = h5_file[dataset_path4]\n",
    "                    dataset5 = h5_file[dataset_path5]\n",
    "                    dataset6 = h5_file[dataset_path6]\n",
    "                    dataset7 = h5_file[dataset_path7]\n",
    "                    \n",
    "                    # Read the data into NumPy arrays\n",
    "                    data = dataset[()]\n",
    "                    data1 = dataset1[()]\n",
    "                    data2 = dataset2[()]\n",
    "                    data3 = dataset3[()]\n",
    "                    data4 = dataset4[()]\n",
    "                    data5 = dataset5[()]\n",
    "                    data6 = dataset6[()]\n",
    "                    data7 = dataset7[()]\n",
    "                    \n",
    "\n",
    "                    # Create a DataFrame from the data arrays\n",
    "                    df = pd.DataFrame({\n",
    "                        'longitude_centroid': data,\n",
    "                        'latitude_centroid': data1,\n",
    "                        'soil moisture': data2,\n",
    "                        'soil_moisture_error': data3,\n",
    "                        'soil_moisture_option1': data4,\n",
    "                        'soil_moisture_option2': data5,\n",
    "                        'latitude': data6,\n",
    "                        'longitude': data7\n",
    "                    })\n",
    "\n",
    "                    # Append the DataFrame to the list\n",
    "                    dfs.append(df)\n",
    "                else:\n",
    "                    print(f\"One or more datasets not found in the HDF5 file: {file_path}\")\n",
    "    file_path=download_path+\"\\\\\"+\"output1.xlsx\"\n",
    "    # Combine all DataFrames into a single DataFrame (row-bind them)\n",
    "    if dfs:\n",
    "        final_df = pd.concat(dfs, ignore_index=True)\n",
    "        #print(final_df)\n",
    "    else:\n",
    "        print(\"No valid HDF5 files found in the folder.\")\n",
    "    \n",
    "    df.to_excel(file_path, index=False)  # Set index=False to exclude row numbers from the output\n",
    "\n",
    "    #print(\"DataFrame has been successfully written to Excel file:\", file_path)\n",
    "    min_longitude = 78\n",
    "    max_longitude = 90\n",
    "    min_latitude = 25\n",
    "    max_latitude = 35\n",
    "\n",
    "    # Create boolean masks for filtering\n",
    "    longitude_mask = (df['longitude_centroid'] >= min_longitude) & (df['longitude_centroid'] <= max_longitude)\n",
    "    latitude_mask = (df['latitude_centroid'] >= min_latitude) & (df['latitude_centroid'] <= max_latitude)\n",
    "\n",
    "    # Use the masks to filter the DataFrame\n",
    "    filtered_df = df[longitude_mask & latitude_mask]\n",
    "    file_path1=download_path+\"\\\\\"+\"Nepal_output.xlsx\"\n",
    "    if len(filtered_df):\n",
    "        single_date=date_str1[:-1]\n",
    "        single_date1 = single_date.replace(\".\", \"-\")\n",
    "        single_date1 = pd.to_datetime(single_date1)\n",
    "        single_date1 = single_date1.strftime('%Y-%m-%d')\n",
    "        # Use .loc to update the 'Date' column\n",
    "        filtered_df.loc[:, 'Date'] = single_date1\n",
    "    else:\n",
    "        filtered_df['Date']=None\n",
    "        #filtered_df['Date'] = single_date1\n",
    "        # Write the DataFrame to an Excel file\n",
    "    filtered_df .to_excel(file_path1, index=False)  # Set index=False to exclude row numbers from the output\n",
    "\n",
    "    #print(\"DataFrame has been successfully written to Excel file:\", file_path)\n",
    "    #print(filtered_df)\n",
    "\n",
    "    #driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_download_path = \"C:\\\\Users\\\\Acer\\\\Desktop\\\\Soil_moisture_SMAP\\\\all_day\"  # Replace this with the base download location\n",
    "\n",
    "    start_date = datetime(2021, 11, 20)\n",
    "    end_date = datetime(2023, 8, 12)\n",
    "\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        date_str = current_date.strftime(\"%Y.%m.%d\")\n",
    "        folder_name = current_date.strftime(\"%Y.%m.%d\") # current_date.strftime(\"%B%d\").lower()  # Create a folder name like \"april10\"\n",
    "        download_path = os.path.join(base_download_path, folder_name)\n",
    "\n",
    "        # Create the folder if it doesn't exist\n",
    "        if not os.path.exists(download_path):\n",
    "            os.makedirs(download_path)\n",
    "\n",
    "        #full_url = \"https://n5eil01u.ecs.nsidc.org/SMAP/SPL2SMP.008/\" + date_str + \"/\"\n",
    "        date_str1=date_str + \"/\"\n",
    "        full_url = \"https://n5eil01u.ecs.nsidc.org/SMAP/SPL2SMP.008/\"\n",
    "        download_h5_files(full_url, download_path,date_str1)\n",
    "        # Code added for deleting the downloaded .h5 file\n",
    "        for filename in os.listdir(download_path):\n",
    "            if filename.endswith('.h5'):\n",
    "                file_path = os.path.join(download_path, filename)\n",
    "                os.remove(file_path)\n",
    "\n",
    "        # Move to the next date\n",
    "        current_date += timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693b44c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9386e5ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
